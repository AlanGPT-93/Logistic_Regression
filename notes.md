# Logistic Regression

## ¬øQu√© es la regresi√≥n log√≠stica?

Es un algorito supervisado que pertece a los algoritmos de clasificaci√≥n. Este no busca entregarnos un valor conituo si no que buscar entregar valores de 0 o 1. Es importante la desambiguaci√≥n porque es cierto que en regresi√≥n lineal buscamos predecir valores continuos. La regresi√≥n logistica en su coraz√≥n se vale de la funci√≥n sigmoidal, que va desde 0 a 1, un intervalo probabilistico para nuestras observaciones.

¬øQu√© probabilidad tiene una observaci√≥n o dato de estar en 0 o en 1?
Podemos hacer una gr√°fica de Probabilidad de Aprobar vs Horas de estudio, y graficor algunos puntos de datos de estudiantes que practican desde 0 a n horas, cada punto de datos se puede mapear a la funci√≥n sigmoide para mapear una probabilidad tal que:

Si est√° entre [0.5-1] el estudiante tiene mejores chances de √©xito.
Si est√° entre [0-0.5) el estudiante tiene menos chances de ganar.
Manjenado los intervalos de probabilidad podemos realizar clasificaciones binarias de si y no.

## Tu primera clasificaci√≥n con regresi√≥n log√≠stica

https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html

https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html

https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html

## ¬øCu√°ndo usar regresi√≥n log√≠stica?

### La Regresi√≥n logistica se utiliza en los siguientes casos:

- Clasificaci√≥n binaria: la regresi√≥n log√≠stica se utiliza para clasificar la observaci√≥n en dos categor√≠as distintas, como ‚Äús√≠‚Äù o ‚Äúno‚Äù, ‚Äú√©xito‚Äù o ‚Äúfracaso‚Äù, ‚Äúcompra‚Äù o ‚Äúno compra‚Äù, etc.

- Datos de entrada no lineales: la regresi√≥n log√≠stica puede manejar datos de entrada no lineales y se puede utilizar para modelar la relaci√≥n entre variables predictoras y variables de respuesta no lineales.

- Datos con valores at√≠picos: la regresi√≥n log√≠stica es robusta a los valores at√≠picos y no se ve afectada significativamente por los puntos de datos que se desv√≠an de la tendencia general.

- Problemas de clasificaci√≥n multiclase: si bien la regresi√≥n log√≠stica es una t√©cnica de clasificaci√≥n binaria, tambi√©n se puede utilizar para problemas de clasificaci√≥n multiclase mediante la t√©cnica ‚Äúuno contra todos‚Äù.

### Sintaxis
La sintaxis de sklearn permite configurar la distribucion de probabilidad o el metodo para resolver el problema
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression

### Ventajas

- F√°cil de implementar: Con sklearn es muy sencillo
- Coeficientes interpretables: Puedo entender los coeficientes y ver c√≥mo se aplican.
- Inferencia: Puedes utilizar distintos features para ver cu√°l tiene m√°s importancia para predecir tu variable dependiente.
- Clasificaci√≥n en porcentajes: **No dice S√≠ y no, te da el porcentaje exacto de confianza {0,100}

### Desventajas

- Asume la linealidad: Asume que todas las relaciones son lineales y no siempre es as√≠.
- Overfitting: Si pongo muchas features, este se aprende el patr√≥n de entrenamiento en lugar de predecirlo.
- Multicolinealidad: Dos caracter√≠sticas que tienen el mismo comportamiento
- Datasets grandes: necesita datasets muy grandes para ser precisos, de lo contrario no alcanza a extraet toda la informaci√≥n.

### ¬øCu√°ndo usarla?
- Sencillo y r√°pido: Cuando esto se busca.
- Prob de ocurrencia de un evento.
- Datasets linealmente separables.
- Datasets grandes.
- Datasets balanceados.

<img src="img/linear_vs_logistic.png" alt="data_engineer_gpc_logo" width="500"/>

## F√≥rmula de regresi√≥n log√≠stica

Los ‚Äúodds‚Äù (en espa√±ol, ‚Äúcuotas‚Äù o ‚Äúprobabilidades‚Äù) son una forma de expresar la probabilidad de que ocurra un evento. En particular, los ‚Äúodds‚Äù representan la relaci√≥n entre la probabilidad de que ocurra un evento y la probabilidad de que no ocurra.

Por ejemplo, si la probabilidad de que un equipo de f√∫tbol gane un partido es del 60%, entonces la probabilidad de que pierda es del 40%. En t√©rminos de ‚Äúodds‚Äù, la probabilidad de ganar se puede expresar como 3 a 2, lo que significa que por cada 2 veces que pierde el equipo, gana 3 veces. De manera similar, la probabilidad de perder se puede expresar como 2 a 3, lo que significa que por cada 3 veces que gana el equipo, pierde 2 veces.

Los ‚Äúodds‚Äù se utilizan com√∫nmente en las apuestas y en los juegos de azar, donde se usan para determinar las ganancias potenciales de una apuesta. En la estad√≠stica, los ‚Äúodds‚Äù se utilizan en la regresi√≥n log√≠stica para modelar la relaci√≥n entre las variables independientes y la variable dependiente binaria.

<img src="img/formula_log_reg.png" alt="data_engineer_gpc_logo" width="500"/>

## Preparando los datos

Antes de realizar cualquier acci√≥n de predicci√≥n se debe:

- Eliminar duplicados.
- Evaluar valores nulos.
- Remover columnas innecesarias.
- Procesar datos categ√≥ricos.
- Remover outliers.
- Escalar Data.

# An√°lisis de correlaci√≥n y escalabilidad de los datos

Hay un concepto en Machine Learning llamado DATA LEAKAGE, que basicamente consiste en que informaci√≥n fuera de los datos de entrenamiento es usada para entrenar el modelo, por ejemplo entrenar un escalador con todos los datos cuando deber√≠a ser solo entrenado con los datos de entrenamiento , esto genera que se puedan obtener resultados muy optimistas al entrenar nuestro modelo,pueden leer sobre eso aqu√≠:
https://machinelearningmastery.com/data-leakage-machine-learning/


# An√°lisis exploratorio de datos

Es recomendable hacerlo con el DF original.
https://platzi.com/cursos/matplotlib-seaborn/



# Entrenamiento con regresi√≥n log√≠stica binomial

- Aplicar Para datos no balanceados

    - Video: https://www.youtube.com/watch?v=4SivdTLIwHc
    - Codigo: https://github.com/dataprofessor/imbalanced-data/blob/main/imbalanced_learn.ipynb
    - Documentaci√≥n methodologies: https://imbalanced-learn.org/stable/references/index.html
    - SMOTE VS SMOTEN: https://datascience.stackexchange.com/questions/60684/smote-vs-smote-nc-for-binary-classifier-with-categorical-and-numeric-data

    - Techniques: https://www.youtube.com/watch?v=GR-OW5asKlk

- Spliting Data

- Rescaling

- Model

# Evaluando el modelo (MLE)- MAXIMUN LIKELIHOOD ESTIMATOR
- M√°s alto MLE mejor est√° haciendo la predicci√≥n.
- Gradient descente disminuye la funci√≥n de costo

# Regularizaci√≥n
Esta t√©cnica consiste en disminuir la complejidad de nuestro modelo a trav√©s de una penalizaci√≥n aplicada a sus variables m√°s irrelevantes.
Para este ejemplo compararemos los resultados obtenidos con los distintos m√©todos de regularizaci√≥n usando la misma semilla y el mismo solver (saga)

## Tipos

- L1 Lasso: Reduce la complejidad a trav√©s de eliminaci√≥n de features que no aportan demasiado al modelo.
Penaliza a los features que aporta poca informaci√≥n volvi√©ndolos cero, eliminado el ruido que producen en el modelo.
        - C es un par√°metro que define que tanto se penaliza a los features que menos aportan, debe ser siempre un valor positivo y mientras mas peque√±o mayor sera la penalizaci√≥n
- L2 Ridge: Reduce la complejidad disminuyendo el impacto de ciertos features a nuestro modelo.
Penaliza los features poco relevantes, pero no los vuelve cero. Solamente limita la informaci√≥n que aportan a nuestro modelo.

## Lasso vs Ridge
1. No hay un campe√≥n definitivo para todos los problemas.
2. Si hay pocos features que se relacionen directamente con la variable a predecir: Probar Lasso.
3. Si hay varios features relacionados con la variable a predecir: Probar Ridge.

Para aplicarlos y decidir cu√°l es el mejor en la pr√°ctica, podemos probar usando alguna t√©cnica como cross-validation iterativamente. o bien, podemos combinarlos‚Ä¶

## ElasticNet: Una t√©cnica intermedia:

Hasta el momento hemos podido ver dos t√©cnicas de regularizaci√≥n en las cuales a√±adimos un componente de penalizaci√≥n en el proceso donde encontramos los valores de los par√°metros ùõΩ minimizando la funci√≥n de error.

Es com√∫n encontrarnos en la literatura con un camino intermedio llamado ElasticNet. Esta t√©cnica consiste en combinar las dos penalizaciones anteriores en una sola funci√≥n.

**Cuando usamos ElasticNet:**

1. Tenemos una forma de probar ambas L1 y L2 al tiempo sin perder informaci√≥n.
2. Supera las limitaciones individuales de ellas.
3. Si hace falta experiencia, o el conocimiento matem√°tico de fondo, puede ser la opci√≥n preferente para probar la regularizaci√≥n.

Para implementar ElasticNet ahora vamos a usar l1_ratio el cual puede tomar valores entre 0 a 1. Si l1_ratio = 0 , ElasticNet se comportar√° como Ridge, y si l1_ratio = 1 , se comportar√° como Lasso. Por lo tanto, nos brinda todo el espectro lineal de posibles combinaciones entre estos dos extremos.

En este caso simplemente vamos a comparar el accuracy que obtenemos con distintos valores de l1_ratio:

# C√≥mo funciona la regresi√≥n log√≠stica multiclase

<img src="img/one_vs_rest.png" alt="one_vs_rest" width="500"/>

<img src="img/softmax.png" alt="softmax" width="500"/>

Documentaci√≥n de donde obtuvo la tabla de solversüòÑ: https://scikit-learn.org/stable/modules/linear_model.html

Par√°metros de la LogReg que aplican a la multicalse, el algoritmo matem√°tico para optimizar descenso de gradiente

<img src="img/solvers.png" alt="solvers" width="500"/>


# F1-Score Considerations

- Important parameter average if multiclass: https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary

# Otros recursos
https://deepnote.com/@mazzaroli/Regresion-Logistica-con-Python-y-scikit-learn-cd6b9628-59c3-4496-8abc-8beb77d9b4ff

- Multicollinearity: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/